{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encodeing\n",
    "categorises data points using one hot encodeing to categorizes into the 5 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_hot_encodeing():\n",
    "    def __init__(self,data,categories):\n",
    "        self.categories = categories\n",
    "        self.data = data\n",
    "        self.Y_one_hot = np.zeros(shape=(data.shape[0],self.categories.shape[0]))\n",
    "        \n",
    "    def encode(self):\n",
    "        for index,cat in enumerate(self.data[:,-1]):\n",
    "            index_cat = np.where(cat == self.categories)\n",
    "            self.Y_one_hot[index][index_cat] = 1\n",
    "        return self.Y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Extraction\n",
    "Itirares through the folder going into every sub folder and using the sub folder name as category tag \n",
    "\n",
    "for the image files in the sub folder. Tag is used in one hot encodeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Extraction_from_file:\n",
    "    def __init__(self,path_folder):\n",
    "        self.path_folder = path_folder\n",
    "        self.image_data = np.empty((28*28))\n",
    "        \n",
    "        \n",
    "    def Get_images(self,image_path):\n",
    "        # function used to extract the image pixel data into an 1D array\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image.getdata(),dtype=np.int32)/256\n",
    "        #append the image data into image data array which will contain all the image data with the category lable \n",
    "        self.image_data.append(image_array)\n",
    "        \n",
    "    def Extract_from_folder(self):\n",
    "        category_name = []\n",
    "        self.image_data = []\n",
    "        #itirate through the main folder\n",
    "        for root, dirs, files in os.walk(self.path_folder):\n",
    "            for name in files:\n",
    "                if name.endswith(\".png\"):\n",
    "                    image_path = os.path.join(root, name)\n",
    "                    self.Get_images(image_path)\n",
    "                    subdir_name = os.path.basename(root)  \n",
    "                    category_name.append(subdir_name)\n",
    "        \n",
    "        self.image_data = np.array(self.image_data)\n",
    "        category_name = np.array(category_name).reshape(-1, 1) \n",
    "        unique_cat_list = np.unique(category_name)\n",
    "        # Combine image data with labels\n",
    "        self.image_data = np.hstack((self.image_data, category_name))\n",
    "        encode_one_hot = One_hot_encodeing(self.image_data,unique_cat_list)\n",
    "        self.Y_one_hot = encode_one_hot.encode()\n",
    "        self.X_images = self.image_data[:,:-1].astype(float)\n",
    "        # Rnadomeise the sequence of data\n",
    "        indices = np.random.permutation(self.X_images.shape[0])\n",
    "        #same indices used to randomise both x and y\n",
    "        self.X_images = self.X_images[indices]\n",
    "        self.Y_one_hot = self.Y_one_hot[indices]\n",
    "        return self.X_images,self.Y_one_hot\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Train\"\n",
    "path_test = \"Test\"\n",
    "Train_file = Image_Extraction_from_file(path_train)\n",
    "Test_file = Image_Extraction_from_file(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train,Y_images_train = Train_file.Extract_from_folder()\n",
    "X_images_test,Y_images_test = Test_file.Extract_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train = np.random.randn(500,784)\n",
    "Y_images_train = np.random.randn(500,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAYER CLASS\n",
    "This class contains the hidden layers.\n",
    "\n",
    "Layers are divided into hidden and output based on their internal activation functions.\n",
    "\n",
    "The hidden layers contain the ReLU activation function, with He initialization of weights in the constructor to prevent extreme values in weights. This helps avoid activation explosion and cost divergence into NaN or infinity, which could cause the loss to eventually increase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers_hidden:\n",
    "    def __init__(self,Weights_no,neuron_no):\n",
    "        self.weights = self.weights = np.random.randn(neuron_no, Weights_no) * np.sqrt(2 / Weights_no) #HE initialization\n",
    "        self.bias = np.random.randn(neuron_no)\n",
    "        \n",
    "    def ReLU(self,x):\n",
    "        return np.maximum(x,0)\n",
    "    \n",
    "    def Process(self,input):\n",
    "        self.input = input\n",
    "        self.y = np.add(np.dot(self.input,self.weights.T) , self.bias) #gives an array of output with no of elements  = no of neuron\n",
    "        self.z = self.ReLU(self.y) #activation function based of the layer being used\n",
    "        self.output_layer_hidden = self.z\n",
    "        return self.z\n",
    "    \n",
    "    def Backward_propogation_hidden(self,dvalue,learning_rate):\n",
    "      \n",
    "        self.dw = np.dot(dvalue.T,self.input)/self.input.shape[0]\n",
    "        self.dx = np.dot(dvalue, self.weights)\n",
    "        self.db = np.sum(dvalue, axis=0) / self.input.shape[0]\n",
    "        self.update_parameters(learning_rate)\n",
    "        \n",
    "        return self.dx\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        self.weights = self.weights + learning_rate * self.dw \n",
    "        self.bias = self.bias + learning_rate * self.db\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAYER OUTPUT\n",
    "He initialization is used.\n",
    "\n",
    "In the output layer, the Softmax function is used to convert the output into probabilities for each category.\n",
    "\n",
    "The Cross-Entropy loss function is used to measure the multi-class classification prediction with true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_output:\n",
    "    def __init__(self,Weights_no,neuron_no):\n",
    "        self.weights = self.weights = np.random.randn(neuron_no, Weights_no) * np.sqrt(2 / Weights_no) #HE initialization\n",
    "        self.bias = np.random.randn(neuron_no)\n",
    "        \n",
    "    \n",
    "    def Softmax(self,x):\n",
    "        x = x - np.max(x, axis=1, keepdims=True)  # Subtract max per row\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    def Process(self,input):\n",
    "        self.input = input\n",
    "        self.y = np.add(np.dot(self.input,self.weights.T) , self.bias)  #gives an array of output with no of elements  = no of neuron this is for output layer\n",
    "        self.z = self.Softmax(self.y)\n",
    "        self.output_layer = self.z\n",
    "        return self.z\n",
    "    \n",
    "    def Cross_entropy_loss(self,Y_actual):\n",
    "        cost = (-(np.sum(np.multiply(Y_actual,(np.log(self.output_layer + 1e-5))))))\n",
    "        normalised_cost = np.sum(cost)/self.input.shape[0]\n",
    "        return normalised_cost\n",
    "    \n",
    "    def Backward_propogation_output(self,yactual,learning_rate):\n",
    "        self.LR = learning_rate\n",
    "        self.dw = np.dot((np.subtract(yactual ,self.output_layer)).T,self.input)\n",
    "        self.dx = np.dot(np.subtract(yactual , self.output_layer),self.weights)\n",
    "        self.db = np.subtract(yactual ,self.output_layer)\n",
    "        self.update_parameters(self.LR)\n",
    "        return self.dx\n",
    "    \n",
    "    \n",
    "    def update_parameters(self,learning_rate):\n",
    "        self.weights = self.weights + learning_rate * self.dw/self.input.shape[0]\n",
    "        self.bias = self.bias + learning_rate * np.sum(self.db,axis =0)/self.input.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST TRAIN\n",
    "## Constructor\n",
    "Used to initialize the objects of classes LayerHidden and LayerOutput. The objects are initialized by implementing the network once, as the layers are interconnected. The number of neurons in layer n-1 is equal to the number of weights in each neuron in layer n. The output layer should have neurons equal to the number of categories.\n",
    "\n",
    "## Training\n",
    "This function executes through layers. The output is fed to the cost function. The normalized loss is used in gradient descent.\n",
    "\n",
    "Gradient descent calls the backpropagation function of the layers. Here, the layers are iterated from backward, starting from the output layer. The model parameters, such as weights and biases, are updated accordingly.\n",
    "\n",
    "This is iterated multiple times with an adequate learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Test_Neural_network:\n",
    "    def __init__(self,layer_info,X_train,Y_train,X_test,Y_test):\n",
    "        self.no_Layers = len(layer_info)-1\n",
    "        self.Layer_size = layer_info\n",
    "        self.Layer = []\n",
    "        self.LR_his = []\n",
    "        self.loss_in_one_itiration = 0\n",
    "        self.Loss = []\n",
    "        self.X_train = X_train\n",
    "        self.Y_train= Y_train\n",
    "        self.X_train_scrap = X_train[0, : ]\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.dw = 0\n",
    "        self.dx =0\n",
    "        self.db = 0\n",
    "        for i in range(self.no_Layers):\n",
    "           \n",
    "            self.Layer.append(Layers_hidden(len(self.X_train_scrap),self.Layer_size[i])) #making of object list and inisializing parameters.\n",
    "            self.X_train_scrap = self.Layer[i].Process(self.X_train_scrap)\n",
    "        self.Layer_last = Layer_output(len(self.X_train_scrap),self.Layer_size[i+1])  \n",
    "        \n",
    "    def execute_through_layers(self,X):\n",
    "        for i in range(self.no_Layers):\n",
    "            X = self.Layer[i].Process(X)\n",
    "        self.X_last = self.Layer_last.Process(X)\n",
    "           \n",
    "    \n",
    "    def gradient_descent(self,Y_test):\n",
    "        self.Y_test_1 = Y_test\n",
    "        n = self.no_Layers\n",
    "        self.dx_last_layer = self.Layer_last.Backward_propogation_output(self.Y_test_1,self.LR)\n",
    "        for j in range(n - 1, -1, -1):\n",
    "            self.dx_last_layer = self.Layer[j].Backward_propogation_hidden(self.dx_last_layer,self.LR)\n",
    "            \n",
    "    def Train_layers(self,itirations,learning_rate):\n",
    "        self.LR = learning_rate\n",
    "        for i in range(itirations):\n",
    "            \n",
    "            self.execute_through_layers(self.X_train) # this function is called to execute through layers each time\n",
    "            self.loss_in_one_itiration =  (self.Layer_last.Cross_entropy_loss(self.Y_train))  # to save the Cross entropy loss in each itiration\n",
    "            self.gradient_descent(self.Y_train)\n",
    "            self.loss_in_one_itiration = np.average(self.loss_in_one_itiration)\n",
    "            self.Loss.append(self.loss_in_one_itiration)\n",
    "            self.LR_his.append(self.LR)\n",
    "            \n",
    "    def Test_model(self,):\n",
    "        \n",
    "        self.execute_through_layers(self.X_test)\n",
    "        self.Y_predict = self.X_last\n",
    "        return self.Y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer info\n",
    "no of neurnos can be constomised by making an array with each element representing number of neurnons in each layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_info = np.array([X_images_train.shape[1],512,256,128,64,32,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with optimum number of neurons and layers for adequate no of itirations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural_network = Train_Test_Neural_network(Layer_info,X_images_train,Y_images_train,None,None)\n",
    "Neural_network.Train_layers(1000,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost VS Epoch\n",
    "Visualizing the Decreasing cost in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(Neural_network.Loss)),Neural_network.Loss,label ='Cost Vs Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedict the test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Neural_network.Test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computes performsnce metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_classes = np.argmax(predict, axis=1)\n",
    "\n",
    "# Convert true one-hot labels to class labels\n",
    "y_true_classes = np.argmax(Y_images_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "recall= recall_score(y_true_classes, y_pred_classes,average='macro')\n",
    "precision = precision_score(y_true_classes, y_pred_classes,average='macro')\n",
    "f1_ = f1_score(y_true_classes, y_pred_classes,average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy = {accuracy}\\nPrecision = {precision}\\nRecall = {recall}\\nF1 score = {f1_}\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
